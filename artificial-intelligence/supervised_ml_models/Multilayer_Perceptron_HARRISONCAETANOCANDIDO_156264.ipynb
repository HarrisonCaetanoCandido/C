{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trabalho 01 - Rede MLP\n",
        "\n",
        "Harrison Caetano Candido                                    RA:156264\n",
        "\n",
        "Implementar uma rede MLP sem utilizar pacotes prontos, como Pytorch, TensorFlow etc. Com a rede implementada, desenvolver dois modelos:\n",
        "- Classificaçao;\n",
        "- Regressão.\n",
        "\n",
        "Avaliar os hiperparâmetros dos modelos variando o número de camadas, número de neurônios e taxas (eta e momentum)."
      ],
      "metadata": {
        "id": "W4IeFRvpPA6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AgEL17iGtw3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MLP:\n",
        "  # aqui fazemos a inicializacao de alguns hiperparametros da rede\n",
        "  def __init__(self, layer_size, learning_rate=0.01, momentum=0):\n",
        "    self.layer_size = layer_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.weights = [np.random.rand(layer_size[i], layer_size[i+1]) - 0.5 for i in range(len(layer_size) - 1)]\n",
        "    self.biases = [np.random.rand(1, layer_size[i+1]) - 0.5 for i in range(len(layer_size) - 1)]\n",
        "    self.velocities = [np.zeros_like(w) for w in self.weights]\n",
        "\n",
        "  # ------------------------------------- funcoes de ativacao -------------------------------------\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def sigmoid_derivative(self, x):\n",
        "    return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.activations = [x]\n",
        "    self.z_values = []\n",
        "\n",
        "    for w, b in zip(self.weights, self.biases):\n",
        "        # Corrigir a multiplicação de matrizes\n",
        "        z = np.dot(self.activations[-1], w) + b\n",
        "        self.z_values.append(z)\n",
        "        # Aplicar a função de ativação sigmoid no somatório\n",
        "        self.activations.append(self.sigmoid(z))\n",
        "\n",
        "    # Retornar o último elemento da lista de ativação\n",
        "    return self.activations[-1]\n",
        "\n",
        "\n",
        "  def backward(self, x, y):\n",
        "    output_error = y - self.activations[-1]\n",
        "    deltas = [output_error * self.sigmoid_derivative(self.z_values[-1])]\n",
        "\n",
        "    # Calcular os deltas para todas as camadas ocultas\n",
        "    for i in range(len(self.weights) - 1, 0, -1):\n",
        "        delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(self.z_values[i - 1])\n",
        "        deltas.insert(0, delta)  # Adicionar delta ao início da lista\n",
        "\n",
        "    # Atualizar pesos e biases\n",
        "    for i in range(len(self.weights)):\n",
        "        gradient = np.dot(self.activations[i].T, deltas[i])\n",
        "        self.velocities[i] = self.momentum * self.velocities[i] + self.learning_rate * gradient\n",
        "        self.weights[i] += self.velocities[i]\n",
        "        self.biases[i] += self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True)\n",
        "\n",
        "  def train(self, x, y, epochs):\n",
        "    for e in range(epochs):\n",
        "      output = self.forward(x)\n",
        "      self.backward(x, y)\n",
        "      if e % 100 == 0:\n",
        "        loss = np.mean(np.square(output - y))\n",
        "        print(f\"Epoch {e}, Loss: {loss}\")\n",
        "\n",
        "def evaluate_hyperparameters(task_type):\n",
        "  if task_type == \"classification\":\n",
        "    # hiperparametros para classificacao\n",
        "    x = np.array([[0, 0], [0, 1], [1, 0], [1,1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "  elif task_type == \"regression\":\n",
        "    x = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
        "    y = x**2\n",
        "\n",
        "  configs = [\n",
        "      {\"layers\": [x.shape[1], 4, 1], \"learning_rate\": 0.1, \"momentum\": 0},\n",
        "      {\"layers\": [x.shape[1], 8, 1], \"learning_rate\": 0.1, \"momentum\": 0.9},\n",
        "      {\"layers\": [x.shape[1], 4, 4, 1], \"learning_rate\": 0.05, \"momentum\": 0.5}\n",
        "  ]\n",
        "\n",
        "\n",
        "  for config in configs:\n",
        "    mlp = MLP(config[\"layers\"], config[\"learning_rate\"], config[\"momentum\"])\n",
        "    mlp.train(x, y, epochs=1000)\n",
        "\n",
        "    for sample in x[:5]:\n",
        "      print(f\"Entrada: {sample}, Saída prevista: {mlp.forward(sample)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('evaluating classification:')\n",
        "evaluate_hyperparameters(\"classification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joyPOMMG-5Xy",
        "outputId": "5259bd50-d7c5-4f66-a060-32a951d4012e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating classification:\n",
            "Epoch 0, Loss: 0.25182411305888786\n",
            "Epoch 100, Loss: 0.2501508508680531\n",
            "Epoch 200, Loss: 0.2501281607056393\n",
            "Epoch 300, Loss: 0.25010776355131803\n",
            "Epoch 400, Loss: 0.2500891001846181\n",
            "Epoch 500, Loss: 0.2500718337173986\n",
            "Epoch 600, Loss: 0.2500556828626769\n",
            "Epoch 700, Loss: 0.2500404088394556\n",
            "Epoch 800, Loss: 0.2500258052716856\n",
            "Epoch 900, Loss: 0.25001169024978265\n",
            "Entrada: [0 0], Saída prevista: [[0.49969199]]\n",
            "Entrada: [0 1], Saída prevista: [[0.51193677]]\n",
            "Entrada: [1 0], Saída prevista: [[0.4872861]]\n",
            "Entrada: [1 1], Saída prevista: [[0.49921764]]\n",
            "Epoch 0, Loss: 0.29056934235035187\n",
            "Epoch 100, Loss: 0.24993714847825166\n",
            "Epoch 200, Loss: 0.24984698585309417\n",
            "Epoch 300, Loss: 0.24967856774293906\n",
            "Epoch 400, Loss: 0.24920410123479356\n",
            "Epoch 500, Loss: 0.2465497787037349\n",
            "Epoch 600, Loss: 0.21892918082141086\n",
            "Epoch 700, Loss: 0.14470901411500936\n",
            "Epoch 800, Loss: 0.06217760660792823\n",
            "Epoch 900, Loss: 0.02310674123912062\n",
            "Entrada: [0 0], Saída prevista: [[0.08548134]]\n",
            "Entrada: [0 1], Saída prevista: [[0.88630378]]\n",
            "Entrada: [1 0], Saída prevista: [[0.89513289]]\n",
            "Entrada: [1 1], Saída prevista: [[0.12215856]]\n",
            "Epoch 0, Loss: 0.25393204462972513\n",
            "Epoch 100, Loss: 0.2500029658331833\n",
            "Epoch 200, Loss: 0.24999846473057932\n",
            "Epoch 300, Loss: 0.24999840586801658\n",
            "Epoch 400, Loss: 0.24999835177952112\n",
            "Epoch 500, Loss: 0.2499982976982419\n",
            "Epoch 600, Loss: 0.2499982436139052\n",
            "Epoch 700, Loss: 0.24999818952133673\n",
            "Epoch 800, Loss: 0.24999813541538152\n",
            "Epoch 900, Loss: 0.24999808129089846\n",
            "Entrada: [0 0], Saída prevista: [[0.49957594]]\n",
            "Entrada: [0 1], Saída prevista: [[0.50071523]]\n",
            "Entrada: [1 0], Saída prevista: [[0.49928295]]\n",
            "Entrada: [1 1], Saída prevista: [[0.50041297]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('evaluating regression:')\n",
        "evaluate_hyperparameters(\"regression\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYoj4Dbz--4Y",
        "outputId": "a1c8ce28-53ca-4e01-dabb-b84dab2d9422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating regression:\n",
            "Epoch 0, Loss: 0.11827800051206366\n",
            "Epoch 100, Loss: 0.09245606985026665\n",
            "Epoch 200, Loss: 0.09243716340338866\n",
            "Epoch 300, Loss: 0.09240133767509828\n",
            "Epoch 400, Loss: 0.09231568045927585\n",
            "Epoch 500, Loss: 0.09199803031234458\n",
            "Epoch 600, Loss: 0.08925099361664696\n",
            "Epoch 700, Loss: 0.06737242404790304\n",
            "Epoch 800, Loss: 0.010879609051634194\n",
            "Epoch 900, Loss: 0.00353548570978942\n",
            "Entrada: [-1.], Saída prevista: [[0.83182695]]\n",
            "Entrada: [-0.97979798], Saída prevista: [[0.82446386]]\n",
            "Entrada: [-0.95959596], Saída prevista: [[0.81597973]]\n",
            "Entrada: [-0.93939394], Saída prevista: [[0.80622246]]\n",
            "Entrada: [-0.91919192], Saída prevista: [[0.79502388]]\n",
            "Epoch 0, Loss: 0.11353590371041195\n",
            "Epoch 100, Loss: 0.02177547941516741\n",
            "Epoch 200, Loss: 0.0015559837951129001\n",
            "Epoch 300, Loss: 0.0014111615693224659\n",
            "Epoch 400, Loss: 0.0013652706740250042\n",
            "Epoch 500, Loss: 0.0013431057804273677\n",
            "Epoch 600, Loss: 0.0013287868579507534\n",
            "Epoch 700, Loss: 0.0013175122466649323\n",
            "Epoch 800, Loss: 0.0013074507785854283\n",
            "Epoch 900, Loss: 0.001297729783173673\n",
            "Entrada: [-1.], Saída prevista: [[0.89227342]]\n",
            "Entrada: [-0.97979798], Saída prevista: [[0.87933733]]\n",
            "Entrada: [-0.95959596], Saída prevista: [[0.86476412]]\n",
            "Entrada: [-0.93939394], Saída prevista: [[0.84840808]]\n",
            "Entrada: [-0.91919192], Saída prevista: [[0.83013433]]\n",
            "Epoch 0, Loss: 0.11122570658821047\n",
            "Epoch 100, Loss: 0.09249667956348945\n",
            "Epoch 200, Loss: 0.09249409320182667\n",
            "Epoch 300, Loss: 0.0924919398074204\n",
            "Epoch 400, Loss: 0.09249005380784057\n",
            "Epoch 500, Loss: 0.09248830943357245\n",
            "Epoch 600, Loss: 0.09248659979910734\n",
            "Epoch 700, Loss: 0.09248482119591267\n",
            "Epoch 800, Loss: 0.09248285802147577\n",
            "Epoch 900, Loss: 0.09248056408997807\n",
            "Entrada: [-1.], Saída prevista: [[0.33975342]]\n",
            "Entrada: [-0.97979798], Saída prevista: [[0.33975851]]\n",
            "Entrada: [-0.95959596], Saída prevista: [[0.33976368]]\n",
            "Entrada: [-0.93939394], Saída prevista: [[0.33976891]]\n",
            "Entrada: [-0.91919192], Saída prevista: [[0.33977422]]\n"
          ]
        }
      ]
    }
  ]
}